In this section, we describe our implementation of WHVI and list assumptions for parts which were not described in detail in the original paper.
We discuss the core classes, FWHT, setting the prior, and initializations for parameters.
\subsection{Core classes}\label{subsec:core-classes}
We implemented WHVI in PyTorch~\cite{pytorch}, because it is also used in the original paper.
The most important part is the \texttt{WHVISquarePow2Matrix} class, which contains as parameters the elements of $\mathbf{S}_1$, $\mathbf{S}_2$, and the posterior mean $\mu$.

The authors do not state how they represent $\Sigma$.
We do not directly use the elements of the diagonal covariance matrix $\Sigma$, but instead use a parameter vector $\rho$ with $D$ elements, which corresponds to $\Sigma$ via a softplus transformation: $\Sigma = \mathrm{diag}(\sigma_1, \dots, \sigma_D)$ where $\sigma_i = \ln(1 + \exp(\rho_i))$.
We choose the softplus transformation, because it was used in the seminal work on variational inference in neural networks~\cite{blundell2015weight}.
The advantage is that we can optimize $\rho_i$ across the real line and disregard positivity constraints in the gradient-based optimization, then ensure non-negativity via a simple transformation.
We acknowledge that the referenced work uses softplus for individual elements of the weight matrix, whereas we use it for elements of vector $\mathbf{g}$, which is indirectly related to the weight matrix via Equation~\ref{eqn:weight-sampling}.

To transform a batch of input vectors, a weight matrix is sampled from the posterior according to Equation~\ref{eqn:weight-sampling} and multiplied by a matrix whose rows are vectors from that batch.
We implemented two options for this transformation.
In the first, we sample the weight matrix directly according to Equation~\ref{eqn:weight-sampling}.
In the second, we use the local re-parameterization trick to sample the matrix-vector product $\mathbf{W}\mathbf{x} \in \mathbb{R}^D$ according to Equations~\ref{eqn:lrt1} and~\ref{eqn:lrt2}.
\begin{align}
    \mathbf{W}\mathbf{x} = \overline{\mathbf{W}}(\mathbf{\mu})\mathbf{x} + \overline{\mathbf{W}}(\mathbf{\Sigma}^{1/2}\mathbf{\epsilon})\mathbf{x}, \; \epsilon \sim \mathrm{N}(\mathbf{0}, \mathbf{I}_D)\label{eqn:lrt1},\\
    \overline{\mathbf{W}}(u) = \mathbf{S}_1 \mathbf{H} \mathrm{diag}(\mathbf{u}) \mathbf{H} \mathbf{S}_2\label{eqn:lrt2}
\end{align}
We use the second option as it is not too slow when compared to the first, but has the advantage of decreasing the variance of stochastic gradients.
For example, the number of epochs per second decreases from 190 to 155 on the toy dataset from Section 3.1 of the original paper.

The \texttt{WHVIStackedMatrix} class represents matrices of arbitrary size.
It identifies how many smaller matrices of type \texttt{WHVISquarePow2Matrix} need to be stacked together to allow for the desired matrix multiplication, as well as the necessary padding of the input vector (see Algorithm 1 in the original report).
These smaller matrices are stored in a \texttt{ModuleList} container as an attribute of \texttt{WHVIStackedMatrix}.
When transforming a batch of input vectors, we generate one sample for each small matrix and concatenate these samples into a large matrix.
The inputs are padded as necessary.

As a special case, we also implement the \texttt{WHVIColumnMatrix} class, which transforms a batch of input vectors with a single element into a batch of output vectors with many elements.
It includes a smaller \texttt{WHVISquarePow2Matrix}, which is sampled at input transformation time and reshaped into a column.
The last few elements of this column may be removed to accommodate for the desired output size.
This method is recommended in the paper and indeed reduces the number of parameters from $O(D)$ to $O(\sqrt{D})$, while also reducing the single-vector transform time complexity from $O(D\log D)$ to $O(\sqrt{D}\log D)$.
Note that by transposing the sampled matrix, this class can be used to map many inputs to a single output.

Finally, we create a \texttt{WHVILinear} layer class, which automatically selects the appropriate matrix based on the desired input and output dimensionalities.
This is analogous to the traditional \texttt{Linear} layer in PyTorch, but also includes the computation of KL divergence $D_\mathrm{KL}\left(q(\mathbf{g}|\mu, \Sigma)\, ||\, p(\mathbf{g})\right)$ from the prior $p$ to the variational posterior $q$.
The KL divergence of a \texttt{WHVILinear} layer is the sum of KL divergence terms for all of its descendants of type \texttt{WHVIStackedMatrix}.

\subsection{Fast Walsh-Hadamard transform}\label{subsec:fast-walsh-hadamard-transform}
An important contribution of the paper is the use of FWHT~\cite{fino1976unified}, which allows for log-linear vector transformation time.
We implemented the transform in Python and C++, both implementations can be used on the CPU and the GPU\@.
We also adapted and tested a CUDA kernel implementation, which is considerably faster than the Python and C++ implementations on the GPU\@.
The performance is described more thoroughly in Section~\ref{fig:compute-performance} with additional comparisons to regular matrix multiplication.

\subsection{Priors and parameter initializations}\label{subsec:priors-and-parameter-initializations}
According to the supplementary material for the original paper, we use a zero-mean prior with fully factorized covariance $\lambda \mathbf{I}$ for a particular layer, i.e. $\mathcal{N}(\mathbf{0}, \mathrm{diag}(\lambda, \dots, \lambda)$ for a chosen $\lambda > 0$.

We believe that the original paper should describe the choices of prior variances in more detail.
Currently, the supplement seems to suggest that a constant $\lambda = 10^{-5}$ was used in all layers of the deep Bayesian networks, but this is likely not the case.
By reconsidering the statement in the supplement, we may interpret it as putting a low prior covariance on the last layer and possibly higher ones on the previous layers.
Good choices of $\lambda$ are thus essential at each layer separately.

The authors did not describe the initialization of $\mathbf{S}_1$, $\mathbf{S}_2$, $\Sigma$.
We draw initial elements of $\mathbf{S}_1$ and $\mathbf{S}_2$ i.i.d.\ from $\mathrm{N}(0, 0.01)$ and initial elements of $\rho$ i.i.d.\ from Uniform$(-3, -2)$.
