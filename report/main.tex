%! Author = David Nabergoj
%! Date = 26/01/2021

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{amsfonts}
\author{David Nabergoj}
\title{Reproducibility report: Walsh-Hadamard Variational Inference for Bayesian Deep Learning}

% Document
\begin{document}
    \maketitle


    \section{Introduction}\label{sec:introduction}
    In this report, we assess the reproducibility of the paper ``Walsh-Hadamard Variational Inference for Bayesian Deep Learning''~\cite{rossi2019walsh}.
    We review the key contributions of the paper in Section~\ref{sec:brief-review-of-the-paper}.
    We state the most important findings to be reproduced and describe our approach to reproduce them in Section~\ref{sec:reproducibility-goals}.
    We describe the implementation details in Section~\ref{sec:implementation-details} and compare our empirical results to those in the original paper in Section~\ref{sec:running-the-experiments}.
    Finally, we conclude the report with an overall reproducibility assessment and discuss the clarity of the paper in Section~\ref{sec:conclusion-and-discussion}.

    Disclaimer: the author of this report was not an expert in the fields of variational inference or random matrix theory at the time of writing.
    The major results of this report are based on multiple readings of the original paper~\cite{rossi2019walsh} and re-runs of the described experiments, as well as brief reading of some key referenced literature~\cite{le2014fastfood, blundell2015weight, fino1976unified, kingma2015variational}.

    \section{Brief review of the original paper}\label{sec:brief-review-of-the-paper}
    The authors propose Walsh-Hadamard Variational Inference (WHVI), where weight matrices in Bayesian neural networks are efficiently re-parameterized to allow for linear space complexity and log-linear time complexity at prediction time.
    The key idea is that weight matrices can be efficiently sampled by
    $$
    \widetilde{\mathbf{W}} = \mathbf{S_1} \mathbf{H} \mathrm{diag}(\widetilde{\mathbf{g}}) \mathbf{H} \mathbf{S_2}, \widetilde{\mathbf{g}} \sim q(\mathbf{g})
    $$.
    Here, $\widetilde{\mathbf{W}}$ is a sample of the weight matrix, $\mathbf{S_1}$ and $\mathbf{S_2}$ are deterministic diagonal matrices whose entries need to be optimized, $\mathbf{H}$ is the Walsh-Hadamard matrix\footnote{It is not explicitly stated whether this matrix should be orthonormal}, and $\widetilde{\mathbf{g}}$ is a sample from the distribution $q$.
    The variational posterior distribution $q$ is a multivariate normal distribution with a diagonal covariance matrix, i.e.\ $q(\mathbf{g}) = \mathcal{N}(\mathbf{mu}, \mathbf{Sigma})$.

    This approach offers an advantage over other approaches like Mean field Gaussian variational inference, because it requires $O(D)$ instead of $O(D^2)$ parameters to represent weight matrices of size $D \times D$.
    Furthermore, the matrix-vector product $\mathbf{H}\mathbf{x}$ can be computed in $O(D \logD)$ time and $O(1)$ space using the in-place version of the Fast Walsh-Hadamard transform.
    The described approach supports matrices of size $D \times D$ where $D = 2^p$ for some $p > 0$ in its most basic form, however it is extended to support matrices of arbitrary size by concatenating smaller square matrices.

    WHVI is applied to a toy example, several regression and classification data sets, and is also tested on image classification tasks using Bayesian convolutional neural networks.


    \section{Reproducibility goals and methodology}\label{sec:reproducibility-goals}
    Our main goal is to implement all required procedures for WHVI and run the described experiments.

    Firstly, we would like to see similar results for the toy univariate regression example section 3.1, Figure 4 of the original report.
    This primarily means obtaining similar uncertainty estimates.
    Next, we will focus on the regression and classification data sets, as listed in Table 3 of the original report.
    We wish to obtain similar WHVI test error and WHVI test MNLL (mean negative log-likelihood) estimates, both the mean and the standard deviation.
    In case our results vary significantly from the original results, we will attempt to tune hyperparameters and report on the necessary changes.
    We will test the method across different random seeds to empirically assess stability and convergence.

    Due to empirically long training times for Bayesian neural networks, complex convolutional neural network architectures and the large number of parameters ($\sim 2.3M$), we will not be considering image clasification experiments in this report.
    We also believe they are not crucial to assessing the quality of the proposed approach.
    This is because the linear layers (not involving convolution operations) are already evaluated using standard regression and classification data sets, whereas the authors report that using WHVI for convolutional filters does not yield interesting results due to the small number of parameters.

    We will attempt to reproduce the findings regarding WHVI inference time to a smaller degree, because the workstation used in the experiments of the original paper is significantly more powerful than the one we used in this reproduction study.

    \section{Implementation details}\label{sec:implementation-details}
    % TODO describe how the models and algorithms were implemented (methods)


    \section{Running the experiments}\label{sec:running-the-experiments}
    % TODO describe the details of how the experiments were run


    \section{Conclusion and discussion}\label{sec:conclusion-and-discussion}
    % TODO conclude with key findings regarding the reproducibility, discuss own results and authors' results.


\end{document}