{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# WHVI Regression experiments\n",
    "\n",
    "This notebook demonstrates the capability of WHVI on some benchmark regression datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from networks import WHVIRegression\n",
    "from layers import WHVILinear\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(net, gamma=0.0005, p=0.3, lr=1e-3):\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda t: (1 + gamma * t)**(-p))\n",
    "    return optimizer\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "# def validate_network(net, X, y, optimizer):\n",
    "#     for _ in range(8):\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "#         train_dataset = CustomDataset(X_train, y_train)\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "#         test_dataset = CustomDataset(X_test, y_test)\n",
    "#         test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "        \n",
    "#         net.train_model(train_loader, optimizer, epochs1=500, epochs2=50000)\n",
    "#         net.eval()\n",
    "#         true_values = []\n",
    "#         predictions = []\n",
    "#         for batch_X, batch_y in test_loader:\n",
    "#             pred_y = net(batch_X)\n",
    "#             true_values.append(batch_y)\n",
    "#             predictions.append(pred_y)\n",
    "#         # Compute test error (MSE)\n",
    "#         # Compute test MNLL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concrete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Fix. var.] KL = 0.00, MNLL = 3533.53: 100%|██████████| 500/500 [00:58<00:00,  8.51it/s]     \n",
      "[Opt. var.] KL = 0.01, MNLL = 197.52: 100%|██████████| 500/500 [00:59<00:00,  8.36it/s]         \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('./datasets/Concrete_Data.xls')\n",
    "X = torch.tensor(df.values[:, :-1].astype(np.float32))\n",
    "y = torch.tensor(df.values[:, -1].astype(np.float32).reshape(-1, 1))\n",
    "    \n",
    "concrete = CustomDataset(X, y)\n",
    "concrete_loader = DataLoader(concrete, batch_size=64)\n",
    "\n",
    "net = WHVIRegression([\n",
    "    WHVILinear(8, 50),\n",
    "    WHVILinear(50, 1)\n",
    "])\n",
    "\n",
    "optimizer = make_optimizer(net)\n",
    "# net.train_model(concrete_loader, optimizer, epochs1=500, epochs2=50000)\n",
    "net.train_model(concrete_loader, optimizer, epochs1=500, epochs2=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KIN8MM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Powerplant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yacht dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
